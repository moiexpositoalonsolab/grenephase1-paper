{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94637a8-2359-4afc-be9d-7afe4eebb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import allel\n",
    "import seaborn as sns\n",
    "import allel\n",
    "import pickle\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c391c5-b97f-4dfe-8301-2c2a4e2c7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_wc = pd.read_csv('../gwas/worldclim_ecotypesdata_sorted_20240517.csv', sep = '\\t')[['ecotypeid','bio1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afaeb032-94f1-47fe-bb1a-03f304a119de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(e_wc['bio1'])\n",
    "train_std = np.std(e_wc['bio1'])\n",
    "\n",
    "##scale it\n",
    "e_wc['bio1'] = (e_wc['bio1'] - np.mean(e_wc['bio1'])) / np.std(e_wc['bio1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee787fb3-3fa0-4496-b6a1-065173df769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites_climate = pd.read_csv('../../grene/data/worldclim_sitesdata.csv')[['site', 'bio1']]\n",
    "sites_climate = pd.read_csv('../key_files/bioclimvars_experimental_sites_era5.csv')[['site', 'bio1']]\n",
    "\n",
    "sites_climate['bio1'] = (sites_climate['bio1'] - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c8c31e-978a-4e07-8f44-d64dbc3a103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract climate data as numpy arrays\n",
    "climate_ecotypes = e_wc['bio1'].to_numpy()\n",
    "climate_sites = sites_climate['bio1'].to_numpy()\n",
    "\n",
    "# Calculate the absolute differences between each pair of ecotype and site\n",
    "climatic_distance_matrix = (climate_ecotypes[:, np.newaxis] - climate_sites)**2\n",
    "\n",
    "# Convert the distance matrix back to a DataFrame for better readability\n",
    "distance_df = pd.DataFrame(climatic_distance_matrix, index=e_wc['ecotypeid'], columns=sites_climate['site'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7c6d3d-6719-41a3-ad80-4a266a7ea8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecotypes_names = distance_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dd04772-dfc1-465b-ac33-8080edcc1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_ecotypes = '/carnegie/nobackup/scratch/xwu/grenet/merged_frequency/merged_ecotype_frequency.txt'\n",
    "#ecotypes_names = pd.read_csv('/carnegie/nobackup/scratch/xwu/grenet/ecotypes_names.txt',header=None)[0].to_list()\n",
    "final_gen = pd.read_csv('../key_files/final_gen.csv')['sample_name']\n",
    "#real_ef = pd.read_csv(path_ecotypes, sep = '\\t',usecols = final_gen)\n",
    "#real_ef.index = ecotypes_names\n",
    "final_gen = final_gen[~final_gen.str.startswith('33_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd1b352-499a-42d1-bef5-cdb6c911f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sites = final_gen.str.split('_').str[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "616746c3-1295-4c5d-b635-f55ec882beda",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gen = final_gen.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0537e7-11cd-4476-92e1-59634f8c7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_ef=pd.read_csv('../leave_1_out/stabilizing_selection_data_scaled_2024Jun18.txt', sep = '\\t')[['log_p1_p0','ecotype', 'site','plot']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5051085-af76-426c-84d1-c384ba15de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ef=pd.read_csv('../key_files/delta_ecotype_freq.txt', sep = '\\t', usecols = final_gen)#[['log_p1_p0','ecotype', 'site','plot']]\n",
    "ef=pd.read_csv('../key_files/merged_ecotype_frequency.txt', sep = '\\t', usecols = final_gen)#[['log_p1_p0','ecotype', 'site','plot']]\n",
    "delta_ef.index = ecotypes_names\n",
    "ef.index = ecotypes_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32765868-7ac3-4ce0-873e-1164a6df9997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79cd4ead-ac25-4486-a7e1-5a40c435a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples = pd.read_csv('../key_files/merged_sample_table.csv')[['site', 'plot', 'generation', 'total_flower_counts']]\n",
    "\n",
    "#samples = samples.groupby(['site', 'plot', 'generation'])['total_flower_counts'].sum().reset_index()\n",
    "\n",
    "#samples['min_perc'] = 1/samples['total_flower_counts']\n",
    "\n",
    "#samples['code'] = samples['site'].astype(str) + '_'  + samples['generation'].astype(str) + '_' + samples['plot'].astype(str) \n",
    "\n",
    "#ecotype_freq = pd.read_csv('../key_files/merged_ecotype_frequency.txt',sep = '\\t')\n",
    "\n",
    "#climate = pd.read_csv('../key_files/bioclimvars_experimental_sites_era5.csv')[['site', 'bio1']]\n",
    "\n",
    "#for i in ecotype_freq.columns:\n",
    "    #min_perc = samples[samples['code'] == i]['min_perc'].values[0]\n",
    "#    total_flower_counts = samples[samples['code'] == i]['total_flower_counts'].values[0]\n",
    "    #ecotype_freq.loc[ecotype_freq[i] < min_perc, i] = np.nan\n",
    "#    ecotype_freq[i] = ecotype_freq[i] * total_flower_counts\n",
    "\n",
    "#ecotype_freq = ecotype_freq[final_gen]\n",
    "\n",
    "#delta_ef = ecotype_freq.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba1fc48-827e-4911-b50f-eebf4dc77bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delta_ef.index = ecotypes_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "54f5f351-9c57-4893-b1ee-da068d5fb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "## P1/P0 ECOTYPE FREQ\n",
    "#real_ef['sample'] = real_ef['site'].astype(str) + '_' + real_ef['plot'].astype(str) \n",
    "#real_ef['p1_over_p0'] = np.exp(real_ef['log_p1_p0'])\n",
    "#real_ef = real_ef.drop(['log_p1_p0', 'site', 'plot'],axis=1)\n",
    "#real_ef = real_ef.pivot_table(index = 'ecotype', columns = 'sample', values='p1_over_p0')\n",
    "#real_ef.index = real_ef.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f3d2b-060b-409a-93d1-8677ac742689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaff332-e9fc-42eb-985f-ad5dd0ff4a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "73d9bbfd-86d8-40a1-b9fc-ec1030336495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_ecotypes = '../stabilizing_selection_data.txt'\n",
    "\n",
    "#real_ef = pd.read_csv(path_ecotypes, sep = '\\t') #,usecols = final_gen)\n",
    "\n",
    "#unique_sites = real_ef['site'].unique()\n",
    "\n",
    "#real_ef['sample'] = real_ef['site'].astype(str) + '_' + real_ef['plot'].astype(str) \n",
    "\n",
    "#real_ef = real_ef[['log_p1_p0', 'ecotype', 'sample']]\n",
    "\n",
    "\n",
    "\n",
    "#real_ef = real_ef.drop(real_ef[real_ef['log_p1_p0'] == -np.inf].index)\n",
    "\n",
    "#real_ef = real_ef.pivot_table(index = 'ecotype', columns = 'sample', values='log_p1_p0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6f7ba5bd-353b-4e3c-97f7-0a959c6192c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#real_ef = real_ef.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8606ce4-d384-496b-a46b-acdc9d776c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fdf393e-19da-46d5-af27-f7e46ccbf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to your .pkl file\n",
    "file_path = '../jacknife_lfmm/splits_samples_last_gen_no33.pkl'\n",
    "\n",
    "# Open and load the .pkl file\n",
    "with open(file_path, 'rb') as file:\n",
    "    splits_samples_first_gen = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdc75416-bbaa-4102-9713-84800f9bd2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits_samples_first_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "849002c3-345c-4ea2-b999-f0a6f62274e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "all_splits_predictions = {}\n",
    "for split in range(len(splits_samples_first_gen)):\n",
    "    print(split)\n",
    "    ## ge thte train and test samples/sites\n",
    "    train = splits_samples_first_gen[split][0]\n",
    "    \n",
    "    test = splits_samples_first_gen[split][1]\n",
    "    ##get the sites to train/test \n",
    "    sites_u_train = pd.Series(train).str.split('_').str[0].unique()\n",
    "    sites_u_test = pd.Series(test).str.split('_').str[0].unique()\n",
    "    # conver tthem to int for easiest manipulations\n",
    "    sites_u_train_int =  [int(i) for i in sites_u_train ]\n",
    "    sites_u_test_int =  [int(i) for i in sites_u_test ]\n",
    "    ## get the necesary cliamtic distances and ecotype freq \n",
    "    ef1 = ef[[col for col in ef.columns if any(col.startswith(f'{site}_') for site in sites_u_train_int)]]\n",
    "    cd = distance_df[sites_u_train_int]\n",
    "\n",
    "    cd_test = distance_df[sites_u_test_int]\n",
    "    cd_test = cd_test.reset_index().melt(id_vars=['ecotypeid'])\n",
    "    \n",
    "    # melt them for easy manipulation\n",
    "    ef_train = ef1.reset_index().melt(id_vars = ['ecotypeid'])\n",
    "    cd_train = cd.reset_index().melt(id_vars = ['ecotypeid'])\n",
    "    \n",
    "    ef_train['site'] = ef_train['variable'].str.split('_').str[0]\n",
    "    ef_train = ef_train.drop('variable',axis=1)\n",
    "    \n",
    "    ef_train.columns = ['ecotypeid', 'ef', 'site']\n",
    "    cd_train.columns = ['ecotypeid', 'site', 'cd']\n",
    "    \n",
    "    cd_train['site'] = cd_train['site'].astype(str)\n",
    "\n",
    "    ## get the full trianign dataset with all ecotypes \n",
    "    train = ef_train.merge(cd_train, on =['site', 'ecotypeid'])\n",
    "\n",
    "    ## predict each ecotype at a time \n",
    "    prediction_ecotypes = {}\n",
    "    for ecotype_1 in train['ecotypeid'].unique():\n",
    "        \n",
    "        model1 = train[train['ecotypeid'] == ecotype_1]\n",
    "        \n",
    "        X = model1['cd'].values.reshape(-1, 1)  # Independent variable\n",
    "        y = model1['ef'].values # Dependent variable\n",
    "        \n",
    "        # Fit the linear model\n",
    "        model = LinearRegression().fit(X, y)\n",
    "        # Predict the valu.es\n",
    "        \n",
    "        cd_ecotype = cd_test[cd_test['ecotypeid'] == ecotype_1]['value'].values[0]\n",
    "        y_pred = model.predict(cd_ecotype.reshape(1, -1) )\n",
    "    \n",
    "        prediction_ecotypes[ecotype_1] = y_pred\n",
    "    \n",
    "    prediction_ecotypes = pd.DataFrame(prediction_ecotypes).T\n",
    "    \n",
    "    all_splits_predictions[sites_u_test[0]] = prediction_ecotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4b01c-843a-4f66-b89b-de42f24803f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ece86e3d-fde0-4651-8631-1712b4ccbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits_predictions = pd.concat(all_splits_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fa9236a-fe6d-4fd6-9394-271f1bb053c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits_predictions.columns = all_splits_predictions.columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a30b294-6908-4c74-a802-ac1372e90084",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean = {}\n",
    "for split in range(len(splits_samples_first_gen)):\n",
    "    ## ge thte train and test samples/sites    \n",
    "    test = splits_samples_first_gen[split][1]\n",
    "    ##get the sites to train/test \n",
    "    sites_u_test = pd.Series(test).str.split('_').str[0].unique()\n",
    "    # conver tthem to int for easiest manipulations\n",
    "    sites_u_test_int =  [int(i) for i in sites_u_test ]\n",
    "    ## get the necesary cliamtic distances and ecotype freq \n",
    "    ef1 = ef[[col for col in ef.columns if any(col.startswith(f'{site}_') for site in sites_u_test_int)]]\n",
    "    ef_mean = ef1.mean(axis=1)\n",
    "    ef_mean.name = 'mean'\n",
    "    ## select the prediciotn to compare \n",
    "    prediction_to_test = all_splits_predictions[sites_u_test[0]]\n",
    "\n",
    "    conc = pd.concat([ef_mean, prediction_to_test],axis=1)\n",
    "    ## becasue some ecotpyes are nan \n",
    "    \n",
    "    X_ranked = conc[sites_u_test[0]]\n",
    "    y_ranked = conc['mean']\n",
    "\n",
    "    sp_correlation, _ = spearmanr(X_ranked, y_ranked)\n",
    "    pearsonr_value = pearsonr(X_ranked, y_ranked)[0]\n",
    "    kendall_tau, _ = kendalltau(X_ranked, y_ranked)\n",
    "\n",
    "    X = conc[sites_u_test[0]].values.reshape(-1, 1)  # Independent variable\n",
    "    y = conc['mean'].values # Dependent variable\n",
    "\n",
    "    # Fit the linear model\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    # Predict the valu.es\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    r_squared = r2_score(y, y_pred)\n",
    "            # Find the top 10 indices for observed and predicted value\n",
    "    res_list = [sp_correlation, pearsonr_value, r_squared, kendall_tau]\n",
    "    for i in [10,20,30,40,50]:\n",
    "        ## TAIL BEcause the most common ecotpyes are at thebottom, sor values does ascending \n",
    "        top_10_observed_indices =  conc['mean'].sort_values().tail(i).index\n",
    "        top_10_predicted_indices = conc[sites_u_test[0]].sort_values().tail(i).index\n",
    "\n",
    "        # Compare top 10 indices\n",
    "        common_top_indices = len(set(top_10_observed_indices).intersection(set(top_10_predicted_indices)))\n",
    "\n",
    "        # Calculate the percentage of common top 10 indices\n",
    "        top_10_match_percentage = (common_top_indices / i) * 100\n",
    "        res_list.append(top_10_match_percentage)\n",
    "    results_mean[sites_u_test[0]] = res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00d4b2c7-ba08-4acc-99e5-ccbefb87dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean = pd.DataFrame(results_mean).T\n",
    "\n",
    "results_mean.columns = ['sp_correlation', 'pearsonr', 'r_squared','kendall_tau', 'top_10','top_20','top_30','top_40','top_50']\n",
    "\n",
    "results_mean['sp_correlation'] = results_mean['sp_correlation'] \n",
    "\n",
    "results_mean['pearsonr'] = results_mean['pearsonr'] \n",
    "\n",
    "\n",
    "results_mean = results_mean.reset_index()\n",
    "\n",
    "results_mean = results_mean.rename(columns = {'index':'site'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54d431-35dd-400c-90a1-450c376cec30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "326d9478-b963-4059-aefa-df4b031d67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean.to_csv('climate_distance_model_results_mean_leave_1_out.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421fc27-bb17-4040-80c6-5788eb6e39e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16142016-04a9-4f52-950f-0daa89e82963",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for split in range(len(splits_samples_first_gen)):\n",
    "    ## ge thte train and test samples/sites    \n",
    "    test = splits_samples_first_gen[split][1]\n",
    "    ##get the sites to train/test \n",
    "    sites_u_test = pd.Series(test).str.split('_').str[0].unique()\n",
    "    # conver tthem to int for easiest manipulations\n",
    "    sites_u_test_int =  [int(i) for i in sites_u_test ]\n",
    "    ## get the necesary cliamtic distances and ecotype freq \n",
    "    ef1 = ef[[col for col in ef.columns if any(col.startswith(f'{site}_') for site in sites_u_test_int)]]\n",
    "\n",
    "    ## select the prediciotn to compare \n",
    "    prediction_to_test = all_splits_predictions[sites_u_test[0]]\n",
    "    for sample in ef1.columns:\n",
    "        \n",
    "        ef_1sample = ef1[sample]\n",
    "        conc = pd.concat([ef_1sample, prediction_to_test],axis=1)\n",
    "        ## becasue some ecotpyes are nan \n",
    "        \n",
    "        X_ranked = conc[sites_u_test[0]]\n",
    "        y_ranked = conc[sample]\n",
    "\n",
    "        sp_correlation, _ = spearmanr(X_ranked, y_ranked)\n",
    "        pearsonr_value = pearsonr(X_ranked, y_ranked)[0]\n",
    "        kendall_tau, _ = kendalltau(X_ranked, y_ranked)\n",
    "\n",
    "        X = conc[sites_u_test[0]].values.reshape(-1, 1)  # Independent variable\n",
    "        y = conc[sample].values # Dependent variable\n",
    "\n",
    "        # Fit the linear model\n",
    "        model = LinearRegression().fit(X, y)\n",
    "        # Predict the valu.es\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        r_squared = r2_score(y, y_pred)\n",
    "                # Find the top 10 indices for observed and predicted values\n",
    "\n",
    "        res_list = [sp_correlation, pearsonr_value, r_squared, kendall_tau]\n",
    "\n",
    "        for i in [10,20,30,40,50]:\n",
    "            top_10_observed_indices = conc[sample].sort_values().tail(i).index\n",
    "            top_10_predicted_indices = conc[sites_u_test[0]].sort_values().tail(i).index\n",
    "    \n",
    "            # Compare top 10 indices\n",
    "            common_top_indices = len(set(top_10_observed_indices).intersection(set(top_10_predicted_indices)))\n",
    "    \n",
    "            # Calculate the percentage of common top 10 indices\n",
    "            top_10_match_percentage = (common_top_indices / i) * 100\n",
    "            res_list.append(top_10_match_percentage)\n",
    "        results[sample] = res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a0eb073-ff00-4e72-b0e6-c1343f9978a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).T\n",
    "\n",
    "results.columns = ['sp_correlation', 'pearsonr', 'r_squared','kendall_tau', 'top_10','top_20','top_30','top_40','top_50']\n",
    "\n",
    "#results.columns = ['sp_correlation', 'pearsonr', 'r_squared', 'top_10_match_percentage', 'kendall_tau']\n",
    "\n",
    "results = results.reset_index()\n",
    "\n",
    "results['site'] = results['index'].str.split('_').str[0]\n",
    "\n",
    "results['plot'] = results['index'].str.split('_').str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93e116f8-b265-4a6c-bdd0-f26e80ccac3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sp_correlation</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>kendall_tau</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_20</th>\n",
       "      <th>top_30</th>\n",
       "      <th>top_40</th>\n",
       "      <th>top_50</th>\n",
       "      <th>site</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_2_2</td>\n",
       "      <td>0.460552</td>\n",
       "      <td>0.358252</td>\n",
       "      <td>0.128344</td>\n",
       "      <td>0.315114</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>32.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_2_3</td>\n",
       "      <td>0.443606</td>\n",
       "      <td>0.118825</td>\n",
       "      <td>0.014119</td>\n",
       "      <td>0.302014</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_3_1</td>\n",
       "      <td>0.270915</td>\n",
       "      <td>0.115559</td>\n",
       "      <td>0.013354</td>\n",
       "      <td>0.185067</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>32.5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_3_4</td>\n",
       "      <td>0.195153</td>\n",
       "      <td>0.060537</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.128020</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_3_5</td>\n",
       "      <td>0.395064</td>\n",
       "      <td>0.277638</td>\n",
       "      <td>0.077083</td>\n",
       "      <td>0.264044</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.5</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>60_1_8</td>\n",
       "      <td>0.437185</td>\n",
       "      <td>0.120607</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>0.291766</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>60_1_9</td>\n",
       "      <td>0.120858</td>\n",
       "      <td>0.069203</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>0.082447</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>17.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>60_1_10</td>\n",
       "      <td>0.321997</td>\n",
       "      <td>0.197681</td>\n",
       "      <td>0.039078</td>\n",
       "      <td>0.222536</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>60_1_11</td>\n",
       "      <td>0.386155</td>\n",
       "      <td>0.227219</td>\n",
       "      <td>0.051628</td>\n",
       "      <td>0.268620</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>32.5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>60_1_12</td>\n",
       "      <td>0.328359</td>\n",
       "      <td>0.159667</td>\n",
       "      <td>0.025494</td>\n",
       "      <td>0.227773</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  sp_correlation  pearsonr  r_squared  kendall_tau  top_10  \\\n",
       "0      1_2_2        0.460552  0.358252   0.128344     0.315114    30.0   \n",
       "1      1_2_3        0.443606  0.118825   0.014119     0.302014    10.0   \n",
       "2      1_3_1        0.270915  0.115559   0.013354     0.185067    10.0   \n",
       "3      1_3_4        0.195153  0.060537   0.003665     0.128020    20.0   \n",
       "4      1_3_5        0.395064  0.277638   0.077083     0.264044    10.0   \n",
       "..       ...             ...       ...        ...          ...     ...   \n",
       "343   60_1_8        0.437185  0.120607   0.014546     0.291766    10.0   \n",
       "344   60_1_9        0.120858  0.069203   0.004789     0.082447    10.0   \n",
       "345  60_1_10        0.321997  0.197681   0.039078     0.222536    30.0   \n",
       "346  60_1_11        0.386155  0.227219   0.051628     0.268620    10.0   \n",
       "347  60_1_12        0.328359  0.159667   0.025494     0.227773    20.0   \n",
       "\n",
       "     top_20     top_30  top_40  top_50 site plot  \n",
       "0      30.0  26.666667    32.5    36.0    1    2  \n",
       "1      25.0  23.333333    30.0    36.0    1    3  \n",
       "2      20.0  20.000000    32.5    42.0    1    1  \n",
       "3      10.0  10.000000    15.0    26.0    1    4  \n",
       "4      20.0  20.000000    22.5    32.0    1    5  \n",
       "..      ...        ...     ...     ...  ...  ...  \n",
       "343    15.0  26.666667    30.0    30.0   60    8  \n",
       "344    10.0  16.666667    17.5    28.0   60    9  \n",
       "345    25.0  20.000000    25.0    30.0   60   10  \n",
       "346    30.0  26.666667    32.5    42.0   60   11  \n",
       "347    40.0  30.000000    35.0    40.0   60   12  \n",
       "\n",
       "[348 rows x 12 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48372ec3-bf30-4bec-8c88-9e31a27d3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('climate_distance_model_results_leave_1_out_ecotype_freq_top.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17629321-be9f-40e9-aeb1-c82d55e0ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NO MODEL JUST NAIVE CORRECLATION, NO INFORMATION ABOUT GRENENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36d25dbb-894d-4acc-8744-c171c83fb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean = {}\n",
    "for site in unique_sites:\n",
    "    ## one site \n",
    "    ## retreive ecotype frequecnies \n",
    "    ef1 = ef[[col for col in ef.columns if col.startswith(f'{site}_')]]\n",
    "    cd = distance_df[int(site)]\n",
    "    ef_f = ef1.mean(axis=1)\n",
    "    ef_f.name = 'mean'\n",
    "    conc = pd.concat([ef_f, cd],axis=1)\n",
    "    conc = conc.sort_values('mean')\n",
    "    ## becasue some ecotpyes are nan \n",
    "    conc = conc.dropna()\n",
    "    X_ranked = conc['mean']\n",
    "    y_ranked = conc[int(site)]\n",
    "\n",
    "    sp_correlation, _ = spearmanr(X_ranked, y_ranked)\n",
    "    pearsonr_value = pearsonr(X_ranked, y_ranked)[0]\n",
    "    kendall_tau, _ = kendalltau(X_ranked, y_ranked)\n",
    "\n",
    "    X = conc[int(site)].values.reshape(-1, 1)  # Independent variable\n",
    "    y = conc['mean'].values # Dependent variable\n",
    "\n",
    "    # Fit the linear model\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    # Predict the valu.es\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    r_squared = r2_score(y, y_pred)\n",
    "            # Find the top 10 indices for observed and predicted values\n",
    "\n",
    "    res_list = [sp_correlation, pearsonr_value, r_squared, kendall_tau]\n",
    "\n",
    "    for i in [10,20,30,40,50]:\n",
    "        # head becasue climatics distance is bigger when thee cotypesshould do worst \n",
    "        top_10_observed_indices = conc['mean'].sort_values().tail(i).index\n",
    "        top_10_predicted_indices = conc[int(site)].sort_values().head(i).index\n",
    "\n",
    "        # Compare top 10 indices\n",
    "        common_top_indices = len(set(top_10_observed_indices).intersection(set(top_10_predicted_indices)))\n",
    "\n",
    "        # Calculate the percentage of common top 10 indices\n",
    "        top_10_match_percentage = (common_top_indices / i) * 100\n",
    "        res_list.append(top_10_match_percentage)\n",
    "    results_mean[site] = res_list\n",
    "    ## retreive climatic distances  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a71bd960-31df-4789-9e6c-014e59c04bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean = pd.DataFrame(results_mean).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88a8e931-def0-4350-987f-d4e5597dfda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_mean.columns = ['sp_correlation', 'pearsonr', 'r_squared','kendall_tau', 'top_10','top_20','top_30','top_40','top_50']\n",
    "\n",
    "results_mean['sp_correlation'] = results_mean['sp_correlation'] *-1\n",
    "\n",
    "results_mean['pearsonr'] = results_mean['pearsonr']  *-1\n",
    "results_mean['kendall_tau'] = results_mean['kendall_tau']  *-1\n",
    "results_mean = results_mean.reset_index()\n",
    "\n",
    "#results.columns = ['sp_correlation', 'pearsonr', 'r_squared', 'top_10_match_percentage', 'kendall_tau']\n",
    "\n",
    "results_mean = results_mean.rename(columns = {'index': 'site'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a04ee92-24a7-42de-aabe-2570ca731e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean.to_csv('naive_climate_distance_top_ecotypes_mean.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f102381-387f-4130-9fc6-1bfd4f661f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for site in unique_sites:\n",
    "    ## one site \n",
    "    ## retreive ecotype frequecnies \n",
    "    ef1 = ef[[col for col in ef.columns if col.startswith(f'{site}_')]]\n",
    "    cd = distance_df[int(site)]\n",
    "    \n",
    "    for sample in ef1.columns:\n",
    "        ef_f = ef1[sample]\n",
    "        conc = pd.concat([ef_f, cd],axis=1)\n",
    "        conc = conc.sort_values(sample)\n",
    "        ## becasue some ecotpyes are nan \n",
    "        conc = conc.dropna()\n",
    "        X_ranked = conc[sample]\n",
    "        y_ranked = conc[int(site)]\n",
    "\n",
    "        sp_correlation, _ = spearmanr(X_ranked, y_ranked)\n",
    "        pearsonr_value = pearsonr(X_ranked, y_ranked)[0]\n",
    "        kendall_tau, _ = kendalltau(X_ranked, y_ranked)\n",
    "\n",
    "        X = conc[int(site)].values.reshape(-1, 1)  # Independent variable\n",
    "        y = conc[sample].values # Dependent variable\n",
    "\n",
    "        # Fit the linear model\n",
    "        model = LinearRegression().fit(X, y)\n",
    "        # Predict the valu.es\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        r_squared = r2_score(y, y_pred)\n",
    "                # Find the top 10 indices for observed and predicted values\n",
    "\n",
    "        res_list = [sp_correlation, pearsonr_value, r_squared, kendall_tau]\n",
    "\n",
    "        for i in [10,20,30,40,50]:\n",
    "            top_10_observed_indices = conc[sample].sort_values().tail(i).index\n",
    "            top_10_predicted_indices = conc[int(site)].sort_values().head(i).index\n",
    "    \n",
    "            # Compare top 10 indices\n",
    "            common_top_indices = len(set(top_10_observed_indices).intersection(set(top_10_predicted_indices)))\n",
    "    \n",
    "            # Calculate the percentage of common top 10 indices\n",
    "            top_10_match_percentage = (common_top_indices / i) * 100\n",
    "            res_list.append(top_10_match_percentage)\n",
    "        results[sample] = res_list\n",
    "    ## retreive climatic distances  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c517f6c9-19ad-4e8f-ab8d-a6b8bd778427",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).T\n",
    "\n",
    "results.columns = ['sp_correlation', 'pearsonr', 'r_squared','kendall_tau', 'top_10','top_20','top_30','top_40','top_50']\n",
    "\n",
    "results['sp_correlation'] = results['sp_correlation'] *-1\n",
    "\n",
    "results['pearsonr'] = results['pearsonr']  *-1\n",
    "results['kendall_tau'] = results['kendall_tau']  *-1\n",
    "results = results.reset_index()\n",
    "\n",
    "#results.columns = ['sp_correlation', 'pearsonr', 'r_squared', 'top_10_match_percentage', 'kendall_tau']\n",
    "\n",
    "results = results.reset_index()\n",
    "\n",
    "results['site'] = results['index'].str.split('_').str[0]\n",
    "\n",
    "results['plot'] = results['index'].str.split('_').str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "91b585af-037b-4521-869e-7f962cb49232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>sp_correlation</th>\n",
       "      <th>pearsonr</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>kendall_tau</th>\n",
       "      <th>top_10</th>\n",
       "      <th>top_20</th>\n",
       "      <th>top_30</th>\n",
       "      <th>top_40</th>\n",
       "      <th>top_50</th>\n",
       "      <th>site</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_2_2</td>\n",
       "      <td>0.408645</td>\n",
       "      <td>0.110037</td>\n",
       "      <td>0.012108</td>\n",
       "      <td>0.293025</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_2_3</td>\n",
       "      <td>0.449537</td>\n",
       "      <td>0.096117</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>0.316596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_3_1</td>\n",
       "      <td>0.503553</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.364814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>37.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1_3_4</td>\n",
       "      <td>0.382439</td>\n",
       "      <td>0.056012</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.263813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>35.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1_3_5</td>\n",
       "      <td>0.349472</td>\n",
       "      <td>0.110924</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>0.245777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>27.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>343</td>\n",
       "      <td>60_1_8</td>\n",
       "      <td>0.251909</td>\n",
       "      <td>0.015499</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.174604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>344</td>\n",
       "      <td>60_1_9</td>\n",
       "      <td>0.045511</td>\n",
       "      <td>0.010238</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.015553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>60_1_10</td>\n",
       "      <td>0.233987</td>\n",
       "      <td>0.071336</td>\n",
       "      <td>0.005089</td>\n",
       "      <td>0.167731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>346</td>\n",
       "      <td>60_1_11</td>\n",
       "      <td>0.643584</td>\n",
       "      <td>0.162564</td>\n",
       "      <td>0.026427</td>\n",
       "      <td>0.500212</td>\n",
       "      <td>10.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>347</td>\n",
       "      <td>60_1_12</td>\n",
       "      <td>0.451116</td>\n",
       "      <td>0.127249</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>0.310198</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>32.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0    index  sp_correlation  pearsonr  r_squared  kendall_tau  \\\n",
       "0          0    1_2_2        0.408645  0.110037   0.012108     0.293025   \n",
       "1          1    1_2_3        0.449537  0.096117   0.009239     0.316596   \n",
       "2          2    1_3_1        0.503553  0.041096   0.001689     0.364814   \n",
       "3          3    1_3_4        0.382439  0.056012   0.003137     0.263813   \n",
       "4          4    1_3_5        0.349472  0.110924   0.012304     0.245777   \n",
       "..       ...      ...             ...       ...        ...          ...   \n",
       "343      343   60_1_8        0.251909  0.015499   0.000240     0.174604   \n",
       "344      344   60_1_9        0.045511  0.010238   0.000105     0.015553   \n",
       "345      345  60_1_10        0.233987  0.071336   0.005089     0.167731   \n",
       "346      346  60_1_11        0.643584  0.162564   0.026427     0.500212   \n",
       "347      347  60_1_12        0.451116  0.127249   0.016192     0.310198   \n",
       "\n",
       "     top_10  top_20     top_30  top_40  top_50 site plot  \n",
       "0      10.0    25.0  20.000000    20.0    22.0    1    2  \n",
       "1       0.0     5.0  20.000000    20.0    34.0    1    3  \n",
       "2       0.0    20.0  26.666667    37.5    40.0    1    1  \n",
       "3       0.0    10.0  23.333333    35.0    42.0    1    4  \n",
       "4       0.0    15.0  20.000000    27.5    24.0    1    5  \n",
       "..      ...     ...        ...     ...     ...  ...  ...  \n",
       "343     0.0    20.0  30.000000    30.0    30.0   60    8  \n",
       "344     0.0    15.0  23.333333    20.0    28.0   60    9  \n",
       "345     0.0    10.0  26.666667    25.0    26.0   60   10  \n",
       "346    10.0    40.0  63.333333    65.0    66.0   60   11  \n",
       "347    10.0    20.0  26.666667    32.5    40.0   60   12  \n",
       "\n",
       "[348 rows x 13 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ed271-44b9-45e2-9f04-137c79ab32cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7ec8c50-3431-41fc-83df-30508835b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('naive_climate_distance_top_ecotypes.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda3bc8-37a1-470d-80ef-d89c2efd360f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c642bf-90b5-481d-87bf-81532b60e4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pipeline_snakemake)",
   "language": "python",
   "name": "pipeline_snakemake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
